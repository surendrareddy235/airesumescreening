
4) FAISS
- Use faiss-cpu, build index per job, do nearest neighbor search between JD embedding and candidate embeddings. Keep index in-memory only.

5) EMBEDDINGS
- Use sentence-transformers: `from sentence_transformers import SentenceTransformer`
- Model: `all-MiniLM-L6-v2` (load once in backend process).

6) DASHBOARD UI (exact as requested)
- Single-page dashboard (Dashboard.jsx) that displays:
- Top Summary Cards row: Tokens Used, Cost, Remaining Balance, Total Candidates (data from aggregated stats endpoint `/jobs/stats`).
- Static Pie Chart (inside card) with segments: 0-50%, 50-80%, 80-90%, 90-100%. Show percent distribution and center label avg match.
- Candidate Table next to pie chart with columns: Name, Email, Phone, Exp, Skills, Match%, Status. Use threshold (85%) to highlight green rows and auto-sort by Match% desc.
- Export Button top-right ‘↓ Export Shortlisted (N)’ downloads only ✅ candidates (Excel/CSV).
- Job Description panel under the summary cards showing the JD used.
- Implement responsive layout (desktop-first).

7) BILLING (Stripe)
- /billing/checkout -> create Stripe Checkout session for the $5 plan.
- Track per-user: `free_trial_remaining` (50 resumes). Once the user uses free_trial_remaining, they must purchase plan to process more resumes.
- After purchase, increment `paid_credits` (can be unlimited or defined; for now set basic plan as 1000 credits).

8) EXPORT
- /export/job/{job_id}/shortlisted -> generate CSV/XLSX using pandas and send as download. Only shortlisted (status ✅) rows included.

9) DATABASE MODELS (minimum)
- User: id, username, email, password_hash, is_verified, free_trial_remaining, paid_credits
- Job: id, user_id, title, status, created_at, tokens_used, cost
- CandidateSummary: id, job_id, name, email, phone, experience_years, skills (csv), match_score, status
- Billing/Payments, Stats tables for aggregated metrics

10) SECURITY & SECRETS
- Create .env.example with:
- DATABASE_URL=mysql+pymysql://USER:PASS@HOST:PORT/DB
- JWT_SECRET, JWT_ALGORITHM
- SMTP_HOST, SMTP_PORT, SMTP_USER, SMTP_PASS, SMTP_FROM
- GROQ_API_KEY
- STRIPE_SECRET_KEY, STRIPE_PUBLISHABLE_KEY
- MAX_FILE_SIZE_MB=8
- SCORE_THRESH=85

11) REQUIREMENTS.TXT (backend)
fastapi
uvicorn[standard]
sqlalchemy
pymysql
python-multipart
python-docx
PyPDF2
sentence-transformers
faiss-cpu
requests
stripe
pandas
openpyxl
passlib[bcrypt]
python-jose[cryptography]
email-validator
aiofiles

12) FRONTEND PACKAGE.JSON (key deps)
react, react-dom, vite, axios, chart.js, react-chartjs-2, react-table, react-dropzone, xlsx

13) ERROR HANDLING
- All endpoints return JSON with { "ok": false, "error": "message" } on failure.
- Frontend shows friendly toast messages.

14) RUN SCRIPTS (in root README)
- Backend:
cd backend
pip install -r requirements.txt
copy .env.example to .env and set values
uvicorn app.main:app --reload --host 0.0.0.0 --port 8000
- Frontend:
cd frontend
npm install
npm run dev

15) COMMENTS & TODOs
- Mark all places where the developer must add real Groq/Stripe/MySQL credentials with TODO comments and `.env` placeholder values.
- Add tests folder skeleton for a couple of critical endpoints (auth, upload).

16) FINAL NOTES FOR DEV WHOSES RUNS THIS PROMPT (developer-friendly)
- Make FAISS index creation modular. For large-scale/production you will replace FAISS in-memory with a proper vector DB (Pinecone/Weaviate/Chroma).
- For privacy, if you want to truly avoid storing resume content, delete everything as soon as job completes. Currently aggregated candidate metadata is stored for export/dashboard — change this in config if you want full deletion.
- Keep Groq calls batched and limit tokens — add throttling logic and error handling.

END TASK: output the full repo with the structure above; ensure endpoints and frontend are wired so that someone can run the backend & frontend locally and process a test upload (include sample test PDF in /samples). Create a README with environment var instructions and how to test the signup->upload->dashboard->export flow.

